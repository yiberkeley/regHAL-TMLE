# -*- coding: utf-8 -*-
"""1D-ulfp.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GZeyT0iwvv3SR8HO9DE2n-i3S5h2rUR7
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import cvxpy as cp
import math
from sklearn.model_selection import KFold


###############################################################################
# 1. Data Generation: Right-Censored Data
###############################################################################
def datgen_univariate_censored(n, return_true=False):
    t_true = np.random.beta(2, 2, size=n)
    c = np.random.uniform(0, 1.2, size=n)
    t_obs = np.minimum(t_true, c)
    delta = (t_true <= c).astype(int)
    df = pd.DataFrame({'ttilde': t_obs, 'delta': delta})
    if return_true:
        df['t_true'] = t_true
    return df

###############################################################################
# 2. Data Expansion & Aggregation for Poisson Regression
###############################################################################
def to_long_data_1d_new(data, time_grid, T_col, event_col):
    data = data.copy().reset_index(drop=True)
    data['subject_id'] = data.index  # assign unique ID per individual
    data['bin'] = data[T_col].apply(lambda t: np.searchsorted(time_grid, t, side='right') - 1)
    rows = []
    for i, row in data.iterrows():
        subj = row['subject_id']
        bin_max = int(row['bin'])
        for b in range(bin_max + 1):
            rec = {
                'subject_id': subj,
                'bin': b,
                'start': time_grid[b],
                'end': time_grid[b+1] if b+1 < len(time_grid) else time_grid[b],
                'delta_interval': 1 if (b == bin_max and row[event_col] == 1) else 0
            }
            rows.append(rec)
    df_long = pd.DataFrame(rows)
    df_long['risk_time'] = df_long['end'] - df_long['start']
    df_long['grid_time'] = df_long['end']
    return df_long

def make_aggregated_poisson_data_1d_new(data, time_grid, T_col, event_col, basis_list=None):
    data_long = to_long_data_1d_new(data, time_grid, T_col, event_col)
    def make_design_matrix(X_array, basis_list):
        n = X_array.shape[0]
        result = []
        for b in basis_list:
            cutoff = b['cutoffs'][0]
            result.append((X_array[:, 0] >= cutoff).astype(float))
        if len(result) == 0:
            return np.zeros((n, 0))
        return np.column_stack(result)
    X_array = data_long[['grid_time']].to_numpy()
    if basis_list is not None:
        design = make_design_matrix(X_array, basis_list)
    else:
        design = X_array
    def row_to_str(vec):
        return ",".join(map(str, vec))
    keys = np.apply_along_axis(row_to_str, 1, design)
    data_long['pattern_key'] = keys
    grouped = data_long.groupby('pattern_key').agg(
        D=('delta_interval', 'sum'),
        RT=('risk_time', 'sum')
    ).reset_index()
    unique_rows = np.unique(design, axis=0)
    unique_keys = np.apply_along_axis(row_to_str, 1, unique_rows)
    str_to_idx = {s: i for i, s in enumerate(unique_keys)}
    y_pois = []
    offset_pois = []
    idx_keep = []
    for _, row in grouped.iterrows():
        pat = row['pattern_key']
        if pat in str_to_idx:
            idx_keep.append(str_to_idx[pat])
            y_pois.append(row['D'])
            offset_pois.append(math.log(row['RT'] + 1e-8))
    if len(idx_keep) == 0:
        return {'X': np.zeros((0, 0)), 'y': np.array([]), 'offset_vec': np.array([])}
    X_pois = unique_rows[np.array(idx_keep), :]
    return {'X': X_pois, 'y': np.array(y_pois), 'offset_vec': np.array(offset_pois)}

###############################################################################
# 3. Basis Functions: Indicator Functions 1{t >= cutoff}
###############################################################################
def create_univariate_basis(data, col_index, smoothness, num_knots, main_term):
    vec_temp = data.iloc[:, col_index].values
    vec_temp = vec_temp[vec_temp != 0]
    if len(np.unique(vec_temp)) <= 2:
        vec_temp = np.array([np.max(np.unique(vec_temp))])
    else:
        quantiles = np.linspace(0, 1, num_knots + 2)
        vec_temp_1 = np.quantile(vec_temp, quantiles)[1:-1]
        vec_temp = np.unique(vec_temp_1)
    basis_list_temp = []
    for v in vec_temp:
        basis_list_temp.append({'cols': [col_index], 'cutoffs': [v], 'orders': smoothness})
    if main_term:
        basis_list_temp.append({'cols': [col_index], 'cutoffs': [0], 'orders': 1})
    return basis_list_temp

###############################################################################
# 4. Fit Poisson Regression with Offset and L1 Norm Constraint
#    (Using inequality: cp.norm1(beta[1:]) <= l1_bound)
###############################################################################
def fit_poisson_with_offset_reg_l1_exact(X, y, offset, l1_bound, scale_data=True):
    n, p = X.shape
    if scale_data:
        means = np.mean(X, axis=0)
        stds = np.std(X, axis=0)
        stds[stds==0] = 1.0
        X_scaled = (X - means) / stds
    else:
        X_scaled = X.copy()
        means = np.zeros(p)
        stds = np.ones(p)
    X_aug = np.hstack([np.ones((n, 1)), X_scaled])
    beta = cp.Variable(p+1)
    eta = offset + X_aug @ beta
    poisson_nll = cp.sum(cp.exp(eta) - cp.multiply(y, eta))
    constraints = [cp.norm1(beta[1:]) <= l1_bound]
    problem = cp.Problem(cp.Minimize(poisson_nll), constraints)
    problem.solve(solver=cp.SCS, verbose=False)
    beta_val = beta.value
    if scale_data:
        beta_adj = beta_val.copy()
        beta_adj[1:] = beta_val[1:] / stds
        beta_adj[0] = beta_val[0] - np.dot(means / stds, beta_val[1:])
    else:
        beta_adj = beta_val
    return beta_adj

###############################################################################
# 5. Cross-Validation for L1 Bound Selection (Individuals stay together)
###############################################################################
def poisson_nll(beta, X, y, offset):
    n = X.shape[0]
    X_aug = np.hstack([np.ones((n, 1)), X])
    eta = offset + X_aug.dot(beta)
    return np.sum(np.exp(eta) - y * eta)

def cv_poisson_with_offset_l1_individuals_1d(data, time_grid, T_col, event_col, basis_list, l1_bound_seq, n_splits=5, scale_data=True):
    kf = KFold(n_splits=n_splits, shuffle=True, random_state=123)
    cv_errors = []
    for l1_bound in l1_bound_seq:
        fold_errors = []
        for train_index, val_index in kf.split(data):
            train_data = data.iloc[train_index]
            val_data = data.iloc[val_index]
            agg_train = make_aggregated_poisson_data_1d_new(train_data, time_grid, T_col, event_col, basis_list)
            if agg_train['X'].shape[0] == 0:
                continue
            beta_hat = fit_poisson_with_offset_reg_l1_exact(agg_train['X'], agg_train['y'], agg_train['offset_vec'], l1_bound, scale_data=scale_data)
            agg_val = make_aggregated_poisson_data_1d_new(val_data, time_grid, T_col, event_col, basis_list)
            if agg_val['X'].shape[0] == 0:
                continue
            error = poisson_nll(beta_hat, agg_val['X'], agg_val['y'], agg_val['offset_vec'])
            fold_errors.append(error)
        cv_errors.append(np.mean(fold_errors))
    best_idx = np.argmin(cv_errors)
    best_l1_bound = l1_bound_seq[best_idx]
    print("CV errors for candidate l1_bounds:", cv_errors)
    print("Selected best l1_bound:", best_l1_bound)
    return best_l1_bound, cv_errors

###############################################################################
# 6. Monte Carlo Survival Simulation Functions (for fitted hazard)
###############################################################################
def make_design_matrix(X, basis_list):
    n = X.shape[0]
    design_list = []
    for b in basis_list:
        cutoff = b['cutoffs'][0]
        design_list.append((X[:, 0] >= cutoff).astype(float))
    if len(design_list) == 0:
        return np.zeros((n, 0))
    return np.column_stack(design_list)

def mc_data_1d(n, coef, basis):
    tau = np.linspace(0, 1, 10001)
    t = np.zeros(n)
    t_happ = np.zeros(n)  # 0 means event not yet occurred
    for i in range(len(tau)-1):
        mid_t = (tau[i] + tau[i+1]) / 2.0
        dt = tau[i+1] - tau[i]
        idx = np.where(t_happ == 0)[0]
        if idx.size > 0:
            temp = np.column_stack([np.full(idx.shape, mid_t)])
            design = make_design_matrix(temp, basis)
            design = np.hstack([np.ones((design.shape[0], 1)), design])
            log_hazard = design.dot(coef)
            prob = 1 - np.exp(-dt * np.exp(log_hazard))
            jumps = np.random.binomial(1, prob)
            t_happ[idx] = jumps
            t[idx] = np.where(jumps == 1, mid_t, t[idx])
    t[t_happ == 0] = 1.1
    return pd.DataFrame({'t': t})

def Estimation_test_1d(coef, basis, grid_scale_survival=0.01):
    l1_norm = np.sum(np.abs(coef[1:]))
    num_basis = len(basis)
    tvals = np.arange(0, 1 + grid_scale_survival/10, grid_scale_survival)
    data_mc = mc_data_1d(10000, coef, basis)
    results = []
    for xv in tvals:
        surv = np.mean(data_mc["t"] > xv)
        results.append([xv, surv])
    df_surv = pd.DataFrame(results, columns=["t", "survival_curve"])
    return {"df_surv": df_surv, "l1_norm": l1_norm, "num_basis": num_basis}

###############################################################################
# 7. Working Model Targeting: Update the CV-selected fit in an augmented model
###############################################################################
def compute_score_matrix(data_obs, beta, augmented_basis, time_grid):
    """
    Compute the individual-level score vector for each subject based on
    the Poisson model. For subject i with observed time t_i and event indicator δ_i,
    we approximate:

        score_i = δ_i X(t_i) - ∫_0^(t_i) exp( X(u)^T β ) X(u) du,

    where X(t) = [1, 1{t >= cutoff_1}, ..., 1{t >= cutoff_p}].
    """
    n = data_obs.shape[0]
    p = len(augmented_basis)
    score_matrix = np.zeros((n, p+1))
    for i, row in data_obs.iterrows():
        t_val = row['ttilde']
        delta = row['delta']
        design_t = np.hstack([1.0, np.array([1.0 if t_val >= b['cutoffs'][0] else 0.0 for b in augmented_basis])])
        grid = np.linspace(0, t_val, num=len(time_grid))
        dt = grid[1] - grid[0] if len(grid) > 1 else t_val
        integrated = np.zeros(p+1)
        for u in grid:
            design_u = np.hstack([1.0, np.array([1.0 if u >= b['cutoffs'][0] else 0.0 for b in augmented_basis])])
            eta_u = np.dot(design_u, beta)
            h_u = np.exp(eta_u)
            integrated += h_u * design_u * dt
        score = delta * design_t - integrated
        score_matrix[i, :] = score
    return score_matrix

# def lasso_projection(A, b, lambda_val):
#     p = A.shape[1]
#     x = cp.Variable(p)
#     c = cp.Variable()  # a scalar offset (free)
#     objective = cp.Minimize(cp.sum_squares(A @ x - c - b) + lambda_val * cp.norm1(x))
#     prob = cp.Problem(objective)
#     prob.solve(solver=cp.SCS, verbose=False)
#     return x.value

def lasso_projection(A, b, lambda_val):
    from sklearn.linear_model import Lasso
    # Create a Lasso model; alpha corresponds to the regularization strength.
    # fit_intercept=True allows the model to learn an intercept (offset).
    model = Lasso(alpha=lambda_val, fit_intercept=True, max_iter=10000)
    model.fit(A, b)
    # Return only the coefficients to mimic the original function's behavior.
    return model.coef_

def working_model_targeting(data_obs, s_list, cv_basis, active_cv_beta_non, intercept, candidate_basis, init_grad, time_grid,
                            max_iter=10, tol=0.005, lambda_val=0.01, step_factor=0.01):
    # Augment CV-selected basis with additional candidate basis (by union, using string representation)
    cv_keys = [str(b) for b in cv_basis]
    augmented_basis = cv_basis.copy()
    for b in candidate_basis:
        if str(b) not in cv_keys:
            augmented_basis.append(b)
    # Initialize augmented coefficients:
    augmented_coef = np.zeros(len(augmented_basis) + 1)
    augmented_coef[0] = intercept
    for i, b in enumerate(augmented_basis):
        if str(b) in cv_keys:
            idx = cv_keys.index(str(b))
            augmented_coef[i+1] = active_cv_beta_non[idx]
        else:
            augmented_coef[i+1] = 0.0
    current_coef = augmented_coef.copy()
    
    n = data_obs.shape[0]
    
    for iter_target in range(max_iter):
        # Step 1: Compute the score matrix and non-intercept part.
        score_matrix = compute_score_matrix(data_obs, current_coef, augmented_basis, time_grid)
        S_nonint = score_matrix[:, 1:]
        design_mat = S_nonint  # real score matrix (non-intercept part)

        # Step 2: For each time grid point, compute the projection vector gamma(s) via lasso.
        proj_coeffs = []
        for j in range(len(s_list)):
            b_vec = init_grad[:, j]
            x_proj = lasso_projection(design_mat, b_vec, lambda_val)
            proj_coeffs.append(x_proj)
        proj_coeffs = np.array(proj_coeffs)  # shape: (len(s_list), num_aug)

        # Step 3: Compute the scalar directions d(s) as the sample mean of S_nonint dot gamma(s).
        directions = []
        for j in range(len(s_list)):
            d_vec = design_mat @ proj_coeffs[j]
            directions.append(np.mean(d_vec))
        directions = np.array(directions)
        print(f"sum PnD^*(s)={np.sum(directions):.4e}")
        print(f"norm PnD^*(s)={np.linalg.norm(directions):.4e}")

        # Step 4: Form the weighted update direction.
        weighted_update = np.sum((directions[:, None] / np.linalg.norm(directions)) * proj_coeffs, axis=0)
        

        
        # Compute the projected efficient influence function approximation:
        # For each subject i, D_i^* = (weighted_update)^T S_nonint[i]
        update_vector = weighted_update  # same as aggregated projection vector
        D_vec = design_mat @ update_vector  # efficient influence approximations per subject
        Pn_D = np.mean(D_vec)
        se_D = np.std(D_vec)
        ratio = np.abs(Pn_D / se_D)
        stop_crit = 1 / (np.sqrt(n) * np.log(n))
        print(f"Iteration {iter_target}: |P_nD^*|/se(D^*) = {ratio:.4e}  (stopping criterion: {stop_crit:.4e})")
        
        ratios = np.array([np.abs(np.mean(S_nonint @ proj_coeffs[j])/np.std(S_nonint @ proj_coeffs[j])) for j in range(len(s_list))])
        max_ratio = np.max(ratios)

        stds = np.array([np.std(S_nonint @ proj_coeffs[j]) for j in range(len(s_list))])
        median_std=np.median(stds)

        # Step 5: Update the non-intercept coefficients.
        new_coef = current_coef.copy()
        new_coef[1:] = current_coef[1:] + step_factor* np.sign(Pn_D) *  weighted_update  # update non-intercept part
        diff = np.linalg.norm(new_coef - current_coef)
        print(f"Delta Targeting iter {iter_target}: step size={step_factor:.4e}, diff={diff:.4e}")


        current_coef = new_coef.copy()
        # Stop when the ratio is below the desired threshold.
        # if max_ratio < stop_crit:
        #     break

        if np.linalg.norm(directions)/ np.sqrt(directions.size) < median_std* stop_crit:
            break

    return current_coef, augmented_basis

###############################################################################
# 7.1 NEW HELPER FUNCTIONS FOR DELTA METHOD TARGETING 
###############################################################################

def compute_gamma_delta_for_s(s, current_coef, augmented_basis, A_inv, integration_points=50):
    """
    Compute the delta-method projection γ₍Δ₎(s) for a given grid point s using
    the closed-form expression:
    
      γ₍Δ₎(s)^T = exp(-∫0^s λ(u) du) * [ -∫0^s exp{X(u)^T β} X(u) du ]^T * A_inv
    
    where A_inv = ( (S_nonint^T S_nonint)/n + η_weak I )^{-1}.
    """
    intercept = current_coef[0]
    beta_non = current_coef[1:]
    if s == 0:
        return np.zeros(len(augmented_basis))
    u_grid = np.linspace(0, s, integration_points)
    design_mat = np.array([[1.0 if u >= b['cutoffs'][0] else 0.0 for b in augmented_basis] for u in u_grid])
    lambda_u = np.exp(intercept + design_mat.dot(beta_non))
    cum_hazard = np.trapz(lambda_u, u_grid)
   
    factor = np.exp(-cum_hazard)
    # Integrate the vector ∫0^s exp{X(u)^T β} X(u) du (for each basis)
    integrated_vector = np.trapz(lambda_u[:, None] * design_mat, u_grid, axis=0)
    gamma_delta = factor * ((- integrated_vector) @ A_inv)  # shape: (p,)
    return gamma_delta

def delta_method_targeting(data_obs, s_list, cv_basis, active_cv_beta_non, intercept, candidate_basis, time_grid,
                           max_iter=10, tol=0.005, eta_weak=1e-4, step_factor=0.01, integration_points=50):
    """
    Delta method targeting routine.
    
    Augments the CV-selected basis with candidate basis, initializes the working 
    model coefficients, and then iteratively updates the non-intercept coefficients 
    using the closed-form γ₍Δ₎(s) computed over a grid s_list.
    """
    # Augment CV-selected basis with candidate basis (using string representation to compare)
    cv_keys = [str(b) for b in cv_basis]
    augmented_basis = cv_basis.copy()
    for b in candidate_basis:
        if str(b) not in cv_keys:
            augmented_basis.append(b)
    # Initialize augmented coefficients:
    augmented_coef = np.zeros(len(augmented_basis) + 1)
    augmented_coef[0] = intercept
    for i, b in enumerate(augmented_basis):
        if str(b) in cv_keys:
            idx = cv_keys.index(str(b))
            augmented_coef[i+1] = active_cv_beta_non[idx]
        else:
            augmented_coef[i+1] = 0.0
    current_coef = augmented_coef.copy()
    n = data_obs.shape[0]
    
    for iter_target in range(max_iter):
        # Compute the individual score matrix using the current working model.
        score_matrix = compute_score_matrix(data_obs, current_coef, augmented_basis, time_grid)
        S_nonint = score_matrix[:, 1:]  # remove intercept part
        
        # Compute the “A–matrix” and its inverse.
        A = (S_nonint.T @ S_nonint) / n + eta_weak * np.eye(S_nonint.shape[1])
        try:
            A_inv = np.linalg.inv(A)
        except np.linalg.LinAlgError:
            A_inv = np.linalg.pinv(A)
            
        # For each s in s_list, compute γ₍Δ₎(s)
        gamma_deltas = []
        for s in s_list:
            gamma_delta = compute_gamma_delta_for_s(s, current_coef, augmented_basis, A_inv, integration_points=integration_points)
            gamma_deltas.append(gamma_delta)
        gamma_deltas = np.array(gamma_deltas)  # shape: (len(s_list), p)
        
        # Compute d(s) = average over individuals of S_nonint dot γ(s)
        directions = np.array([np.mean(S_nonint @ gamma_deltas[j]) for j in range(len(s_list))])
        print(f"sum PnD^*(s)={np.sum(directions)}")
        print(f"norm PnD^*(s)={np.linalg.norm(directions)}")
        # Aggregate the update direction across s using the normalized d(s) weights.
        weighted_update = np.sum((directions[:, None] / np.linalg.norm(directions)) * gamma_deltas, axis=0)
        
        # Compute the projected efficient influence function approximation:
        # For each subject i, D_i^* = (weighted_update)^T S_nonint[i]
        update_vector = weighted_update  # same as aggregated projection vector
        D_vec = S_nonint @ update_vector  # efficient influence approximations per subject
        Pn_D = np.mean(D_vec)
        se_D = np.std(D_vec)
        ratio = np.abs(Pn_D / se_D)
        stop_crit = 1 / (np.sqrt(n) * np.log(n))
        print(f"Iteration {iter_target}: |P_nD^*|/se(D^*) = {ratio:.4e}  (stopping criterion: {stop_crit:.4e})")
        
        ratios = np.array([np.abs(np.mean(S_nonint @ gamma_deltas[j])/np.std(S_nonint @ gamma_deltas[j])) for j in range(len(s_list))])
        max_ratio = np.max(ratios)

        stds = np.array([np.std(S_nonint @ gamma_deltas[j]) for j in range(len(s_list))])
        median_std=np.median(stds)

        # Step 5: Update the non-intercept coefficients.
        new_coef = current_coef.copy()
        new_coef[1:] = current_coef[1:] + step_factor* np.sign(Pn_D) *  weighted_update  # update non-intercept part
        diff = np.linalg.norm(new_coef - current_coef)
        print(f"Delta Targeting iter {iter_target}: step size={step_factor:.4e}, diff={diff:.4e}")

        current_coef = new_coef.copy()
        # Stop when the max ratio is below the desired threshold.
        # if max_ratio < stop_crit:
        #     break
        
        if np.linalg.norm(directions)/ np.sqrt(directions.size) < median_std* stop_crit:
            break
    return current_coef, augmented_basis

###############################################################################
# 7-3: Relaxed/GLM Based Update
###############################################################################
def glm_relaxed_targeting(data_obs, current_coef, augmented_basis, time_grid):
    """
    Perform a relaxed (GLM-based) update of the working model coefficients.
    
    This function creates the aggregated Poisson data using the given augmented basis,
    then re-fits a Poisson GLM (without penalization, by setting a very high L1-bound)
    to obtain updated coefficients.
    
    Parameters:
      data_obs: DataFrame with observed data (must contain 'ttilde' and 'delta').
      current_coef: Current coefficient vector (not used as an offset here, but can be used
                    for initialization if desired).
      augmented_basis: List of basis dictionaries (augmented working model basis).
      time_grid: Grid of time points for data expansion.
    
    Returns:
      updated_coef: Updated coefficient vector (from the relaxed/GLM update).
    """
    # Create aggregated data using the augmented basis
    agg_data = make_aggregated_poisson_data_1d_new(data_obs, time_grid, T_col='ttilde', event_col='delta', basis_list=augmented_basis)
    # Set a very high L1-bound so that the penalty is effectively inactive (relaxed update)
    high_l1_bound = 1e10
    updated_coef = fit_poisson_with_offset_reg_l1_exact(agg_data['X'], agg_data['y'], agg_data['offset_vec'], high_l1_bound, scale_data=True)
    return updated_coef

###############################################################################
# 7.2 Confidence Interval Computation for Survival Estimates
###############################################################################
# def cv_lasso_projection(A, b, lambda_seq, cv_folds=5):
#     """
#     Perform cross-validation to select the best lambda for the lasso projection.
    
#     Parameters:
#       A : array-like, shape (n, p)
#           The design matrix.
#       b : array-like, shape (n,)
#           The response vector.
#       lambda_seq : array-like
#           Candidate lambda values to test.
#       cv_folds : int
#           Number of folds for cross-validation.
    
#     Returns:
#       best_x : array, shape (p,)
#           The lasso projection computed on the full data using the selected lambda.
#       best_lambda : float
#           The lambda value that minimized the CV error.
#       cv_errors : array, shape (len(lambda_seq),)
#           The average CV error for each candidate lambda.
#     """
#     from sklearn.model_selection import KFold
#     import numpy as np
    
#     n = A.shape[0]
#     kf = KFold(n_splits=cv_folds, shuffle=True, random_state=123)
#     cv_errors = np.zeros(len(lambda_seq))
    
#     for i, lam in enumerate(lambda_seq):
#         fold_errors = []
#         for train_idx, val_idx in kf.split(A):
#             A_train = A[train_idx, :]
#             b_train = b[train_idx]
#             A_val = A[val_idx, :]
#             b_val = b[val_idx]
#             # Fit using the lasso_projection function
#             x_train = lasso_projection(A_train, b_train, lam)
#             # Use the fitted x_train for prediction (ignoring the offset)
#             pred = A_val @ x_train
#             error = np.mean((pred - b_val)**2)
#             fold_errors.append(error)
#         cv_errors[i] = np.mean(fold_errors)
    
#     best_idx = np.argmin(cv_errors)
#     best_lambda = lambda_seq[best_idx]
#     # Refit on full data with the best lambda
#     best_x = lasso_projection(A, b, best_lambda)
#     return best_x, best_lambda, cv_errors


def cv_lasso_projection(A, b, lambda_seq, cv_folds=5):
    """
    Perform cross-validation to select the best lambda for the lasso projection using LassoCV.
    
    Parameters:
      A : array-like, shape (n, p)
          The design matrix.
      b : array-like, shape (n,)
          The response vector.
      lambda_seq : array-like
          Candidate lambda (alpha) values to test.
      cv_folds : int, default=5
          Number of folds for cross-validation.
    
    Returns:
      best_x : array, shape (p,)
          The lasso coefficients computed on the full data using the selected lambda.
      best_lambda : float
          The lambda (alpha) value that minimized the CV error.
      cv_errors : array, shape (len(lambda_seq),)
          The average CV mean squared error for each candidate lambda.
    """
    from sklearn.linear_model import LassoCV
    import numpy as np

    # Initialize and fit LassoCV.
    # The 'alphas' parameter takes our candidate lambda_seq.
    model = LassoCV(alphas=lambda_seq, cv=cv_folds, max_iter=10000,
                     fit_intercept=True, random_state=123)
    model.fit(A, b)
    
    best_x = model.coef_
    best_lambda = model.alpha_
    # cv_mse_path_ is an array of shape (n_alphas, n_folds); we average across folds.
    cv_errors = np.mean(model.mse_path_, axis=1)
    
    return best_x, best_lambda, cv_errors


def compute_survival_prob(s, current_coef, augmented_basis, integration_points=100):
    """
    Estimate survival probability S(s) = exp(-∫0^s λ(u) du)
    given current coefficients and basis.
    """
    intercept = current_coef[0]
    beta_non = current_coef[1:]
    if s == 0:
        return 1.0
    u_grid = np.linspace(0, s, integration_points)
    # For each u, the design is the vector of indicators for each basis.
    design_mat = np.array([[1.0 if u >= b['cutoffs'][0] else 0.0 for b in augmented_basis] for u in u_grid])
    lambda_u = np.exp(intercept + design_mat.dot(beta_non))
    cum_hazard = np.trapz(lambda_u, u_grid)
    return np.exp(-cum_hazard)


def compute_confidence_intervals(data_obs, current_coef, augmented_basis, s_list, method="reg",
                                 init_grad=None, lambda_val=0.01, eta_weak=1e-4, integration_points=50):
    """
    For each grid point s in s_list, compute the estimated survival probability
    (using the working model fit) and a Wald-type 95% confidence interval based on
    the EIC approximation.
    
    For the projection-based method ("reg"), the γ(s) is computed via the lasso
    projection (using the provided init_grad). For the delta-method ("delta"), the
    closed-form γ₍Δ₎(s) is computed.
    """
    # First, compute the individual score matrix at the current working model.
    score_matrix = compute_score_matrix(data_obs, current_coef, augmented_basis, time_grid=np.linspace(0,1,100))
    S_nonint = score_matrix[:, 1:]  # shape (n, p)
    n = data_obs.shape[0]
    ci_results = []
    
    if method == "reg":
        # Use the lasso-based projection (requires init_grad provided)
        gamma_list = []
        for j, s in enumerate(s_list):
            b_vec = init_grad[:, j]
            gamma_proj = lasso_projection(S_nonint, b_vec, lambda_val)
            gamma_list.append(gamma_proj)
        gamma_array = np.array(gamma_list)  # shape (len(s_list), p)
    
    elif method == "reg_cv":
        # Use cross-validated lasso projection
        # Define a candidate lambda sequence (adjust as needed)
        lambda_seq = np.linspace(0.001, 0.1, 10)
        gamma_list = []
        for j, s in enumerate(s_list):
            b_vec = init_grad[:, j]
            gamma_proj, best_lambda, cv_errors = cv_lasso_projection(S_nonint, b_vec, lambda_seq, cv_folds=5)
            # Optionally, print or log the selected lambda and CV error.
            # print(f"s={s:.3f}, best lambda={best_lambda:.4f}, CV error={np.min(cv_errors):.4e}")
            gamma_list.append(gamma_proj)
        gamma_array = np.array(gamma_list)

    elif method == "delta":
        # Use the delta-method closed form
        A = (S_nonint.T @ S_nonint) / n + eta_weak * np.eye(S_nonint.shape[1])
        try:
            A_inv = np.linalg.inv(A)
        except np.linalg.LinAlgError:
            A_inv = np.linalg.pinv(A)
        gamma_list = []
        for s in s_list:
            gamma_delta = compute_gamma_delta_for_s(s, current_coef, augmented_basis, A_inv, integration_points=integration_points)
            gamma_list.append(gamma_delta)
        gamma_array = np.array(gamma_list)
    
    # For each s, compute the survival estimate and the EIC-based variance.
    for j, s in enumerate(s_list):
        surv_est = compute_survival_prob(s, current_coef, augmented_basis, integration_points=integration_points)
        # For each subject i, compute EIC_i(s) = γ(s)^T * S_nonint[i]
        EIC_values = S_nonint @ gamma_array[j]
        var_est = np.mean(EIC_values**2) / n
        ci_lower = surv_est - 1.96 * np.sqrt(var_est)
        ci_upper = surv_est + 1.96 * np.sqrt(var_est)
        ci_results.append([s, surv_est, ci_lower, ci_upper])
    ci_df = pd.DataFrame(ci_results, columns=["s", "survival_est", "ci_lower", "ci_upper"])
    return ci_df


###############################################################################
# 8. Kaplan-Meier Estimator Function
###############################################################################
def kaplan_meier_estimator(data, time_grid=None):
    """
    Compute the Kaplan-Meier survival curve using observed times (ttilde) and event indicator (delta).
    
    If time_grid is provided, the function returns survival estimates at the grid points.
    Otherwise, it returns estimates at the unique event times.
    """
    df = data.copy().sort_values(by='ttilde')
    times = df['ttilde'].values
    events = df['delta'].values
    current_surv = 1.0
    km_times = []
    km_surv = []
    unique_times = np.unique(times)
    
    # Compute the KM survival estimates at unique times
    for t in unique_times:
        at_risk = np.sum(times >= t)
        events_at_t = np.sum((times == t) & (events == 1))
        if at_risk > 0:
            current_surv = current_surv * (1 - events_at_t / at_risk)
        km_times.append(t)
        km_surv.append(current_surv)
    
    # If a time_grid is provided, map the KM estimates onto the grid
    if time_grid is not None:
        survival_grid = []
        for t in time_grid:
            # Find the last KM time that is <= current grid time
            mask = np.array(km_times) <= t
            if np.any(mask):
                last_idx = np.where(mask)[0][-1]
                survival_grid.append(km_surv[last_idx])
            else:
                survival_grid.append(1.0)  # no event has occurred yet at this time
        return pd.DataFrame({'t': time_grid, 'KM': survival_grid})
    else:
        return pd.DataFrame({'t': km_times, 'KM': km_surv})


###############################################################################
# 9. Main: Complete Workflow with Working Model Targeting and Survival Plot
###############################################################################
def simulation_run(sim, n_samples):
    # Optionally, set a unique seed per simulation so the results differ.
    np.random.seed(4250 + sim)
    n = n_samples
    # Generate right-censored data (with true event times)
    data = datgen_univariate_censored(n, return_true=True)

    # --- Hazard for T ---
    # Define time grid (50 intervals between 0 and 1)
    time_grid = np.linspace(0, 1, 100)
    # Create basis functions for T (observed time column 0)
    basis_list_T = create_univariate_basis(data, col_index=0, smoothness=0, num_knots=100, main_term=False)
    # Candidate L1-bound values for CV for T hazard model
    l1_bound_seq_T = np.linspace(0.1, 3.0, 15)
    best_l1_bound_T, cv_errors_T = cv_poisson_with_offset_l1_individuals_1d(
        data, time_grid, T_col='ttilde', event_col='delta', basis_list=basis_list_T,
        l1_bound_seq=l1_bound_seq_T, n_splits=5, scale_data=True)
    print("Best L1-bound for T hazard:", best_l1_bound_T)
    agg_data_T = make_aggregated_poisson_data_1d_new(data, time_grid, T_col='ttilde', event_col='delta', basis_list=basis_list_T)
    beta_est_T = fit_poisson_with_offset_reg_l1_exact(agg_data_T['X'], agg_data_T['y'], agg_data_T['offset_vec'], best_l1_bound_T, scale_data=True)
    # print("Fitted coefficients for T hazard:", beta_est_T)
    threshold = 1e-4
    active_indices = []
    beta_trunc = beta_est_T.copy()
    for i in range(1, len(beta_est_T)):
        if abs(beta_trunc[i]) < threshold:
            beta_trunc[i] = 0
        else:
            active_indices.append(i-1)
    # print("Truncated coefficients (non-intercept):", beta_trunc[1:])
    # print("Active basis indices after truncation:", active_indices)
    active_basis_set = [basis_list_T[i] for i in active_indices]
    active_cv_beta_non = [beta_trunc[1:][i] for i in range(len(beta_trunc[1:])) if i in active_indices]
    intercept = beta_trunc[0]
    # print("CV-selected working model basis set:", active_basis_set)

    cv_fit_coef = np.concatenate(([intercept], np.array(active_cv_beta_non)))
    cv_fit_basis = active_basis_set.copy()
    
    # --- Candidate Models ---
    m = 5
    #*(1 ** np.arange(2))
    candidate_l1_bounds = best_l1_bound_T * 1.1 * (1.1 ** np.arange(m)) 
    nested_basis_sets = []
    for l1 in candidate_l1_bounds:
        beta_temp = fit_poisson_with_offset_reg_l1_exact(agg_data_T['X'], agg_data_T['y'], agg_data_T['offset_vec'], l1, scale_data=True)
        beta_temp_trunc = beta_temp.copy()
        active_inds = []
        for i in range(1, len(beta_temp)):
            if abs(beta_temp_trunc[i]) < threshold:
                beta_temp_trunc[i] = 0
            else:
                active_inds.append(i-1)
        active_basis = [basis_list_T[i] for i in active_inds]
        nested_basis_sets.append(active_basis)
        print(f"Candidate l1_bound = {l1:.4f}, active basis indices: {active_inds}")
    cv_selected_basis = active_basis_set
    candidate_working_model = nested_basis_sets[-1]
    # print("Candidate augmented working model basis set:", candidate_working_model)

    # --- Hazard for C ---
    # Fit censoring hazard model to compute P(C>t)
    data_censor = data.copy()
    data_censor['alpha1'] = 1 - data_censor['delta']
    basis_list_C = create_univariate_basis(data_censor, col_index=0, smoothness=0, num_knots=50, main_term=False)
    l1_bound_seq_C = np.linspace(0.1, 2.0, 10)
    best_l1_bound_C, cv_errors_C = cv_poisson_with_offset_l1_individuals_1d(
        data_censor, time_grid, T_col='ttilde', event_col='alpha1', basis_list=basis_list_C,
        l1_bound_seq=l1_bound_seq_C, n_splits=5, scale_data=True)
    agg_data_C = make_aggregated_poisson_data_1d_new(data_censor, time_grid, T_col='ttilde', event_col='alpha1', basis_list=basis_list_C)
    beta_est_C = fit_poisson_with_offset_reg_l1_exact(agg_data_C['X'], agg_data_C['y'], agg_data_C['offset_vec'], best_l1_bound_C, scale_data=True)
    # print("Fitted censoring hazard coefficients:", beta_est_C)

    # --- Initial Gradient Part 1---
    def product_integral_C(t_val, coef, basis_list, time_grid):
        grid = np.linspace(0, t_val, num=len(time_grid))
        dt = grid[1] - grid[0]
        hazards = []
        for u in grid:
            design_u = np.array([1.0 if u >= b['cutoffs'][0] else 0.0 for b in basis_list])
            X_u = np.hstack([1.0, design_u])
            h_u = np.exp(np.dot(X_u, coef))
            hazards.append(h_u)
        hazards = np.array(hazards)
        cum_hazard = np.sum(hazards * dt)
        return np.exp(-cum_hazard)
    P_C = []
    for i, row in data.iterrows():
        t_val = row['ttilde']
        P_C.append(product_integral_C(t_val, beta_est_C, basis_list_C, time_grid))
    data['P_C'] = P_C
    s_list = np.linspace(0, 1, 50)
    init_grad = np.zeros((len(data), len(s_list)))
    for i, row in data.iterrows():
        for j, s in enumerate(s_list):
            init_grad[i, j] = (1 if (row['delta'] == 1 and row['ttilde'] > s) else 0) / row['P_C']

    # --- Working Model Targeting: Projection-Based (Existing) ---
    time_grid_score = np.linspace(0, 1, 200)
    updated_coef_reg, updated_basis_reg = working_model_targeting(data, s_list, cv_selected_basis, active_cv_beta_non, intercept,
                                                           candidate_working_model,
                                                           init_grad, time_grid=time_grid_score,
                                                           max_iter=200, tol=0.005,
                                                           lambda_val=1e-5, step_factor=1e-3)

    print("Projection-based targeting completed.")

    # --- Delta Method Based Targeting (New) ---
    updated_coef_delta, updated_basis_delta = delta_method_targeting(data, s_list, cv_selected_basis, active_cv_beta_non, intercept,
                                                                     candidate_working_model, time_grid=time_grid_score,
                                                                     max_iter=200, tol=0.005, eta_weak=1e-6, step_factor=1e-3)
    
    print("Delta-method targeting completed.")

    # --- Relaxed/GLM Based Targeting (New) ---
    # Form the union of the CV-selected basis and candidate working model
    augmented_basis = cv_selected_basis.copy()
    for b in candidate_working_model:
        if str(b) not in [str(x) for x in augmented_basis]:
            augmented_basis.append(b)
    updated_coef_relax = glm_relaxed_targeting(data, updated_coef_reg, augmented_basis, time_grid=time_grid_score)
    updated_basis_relax = augmented_basis
    
    print("Relaxed/GLM-based targeting completed.")

    # --- Survival Estimation ---
    # CV-selected hazard (pre-targeting)
    est_results_cv = Estimation_test_1d(beta_est_T, basis_list_T, grid_scale_survival=0.01)
    df_surv_est_cv = est_results_cv["df_surv"]
    # Projection-based targeted survival
    df_surv_est_reg = Estimation_test_1d(updated_coef_reg, updated_basis_reg, grid_scale_survival=0.01)["df_surv"]
    # Delta-based targeted survival
    df_surv_est_delta = Estimation_test_1d(updated_coef_delta, updated_basis_delta, grid_scale_survival=0.01)["df_surv"]
    # Relaxed/GLM targeted survival
    df_surv_est_relax = Estimation_test_1d(updated_coef_relax, updated_basis_relax, grid_scale_survival=0.01)["df_surv"]
    # Kaplan-Meier survival estimator from raw data
    km_df = kaplan_meier_estimator(data, time_grid=df_surv_est_cv['t'].values)
    
    # --- Inference: Compute 95% Confidence Intervals using the approximated EIC
    # For the initial CV fit:
    cv_CI_REG    = compute_confidence_intervals(data, cv_fit_coef, cv_fit_basis, s_list, method="reg", init_grad=init_grad, lambda_val=1e-5)
    cv_CI_REG_CV = compute_confidence_intervals(data, cv_fit_coef, cv_fit_basis, s_list, method="reg_cv", init_grad=init_grad)
    cv_CI_DELTA  = compute_confidence_intervals(data, cv_fit_coef, cv_fit_basis, s_list, method="delta", eta_weak=1e-6)
    
    # For the projection-based targeting:
    reg_CI_REG    = compute_confidence_intervals(data, updated_coef_reg, updated_basis_reg, s_list, method="reg", init_grad=init_grad, lambda_val=1e-5)
    reg_CI_REG_CV = compute_confidence_intervals(data, updated_coef_reg, updated_basis_reg, s_list, method="reg_cv", init_grad=init_grad)
    reg_CI_DELTA  = compute_confidence_intervals(data, updated_coef_reg, updated_basis_reg, s_list, method="delta", eta_weak=1e-6)
    
    # For the delta-based targeting:
    delta_CI_REG    = compute_confidence_intervals(data, updated_coef_delta, updated_basis_delta, s_list, method="reg", init_grad=init_grad, lambda_val=1e-5)
    delta_CI_REG_CV = compute_confidence_intervals(data, updated_coef_delta, updated_basis_delta, s_list, method="reg_cv", init_grad=init_grad)
    delta_CI_DELTA  = compute_confidence_intervals(data, updated_coef_delta, updated_basis_delta, s_list, method="delta", eta_weak=1e-6)
    
    # For the relaxed/GLM based targeting:
    relax_CI_REG    = compute_confidence_intervals(data, updated_coef_relax, updated_basis_relax, s_list, method="reg", init_grad=init_grad, lambda_val=1e-5)
    relax_CI_REG_CV = compute_confidence_intervals(data, updated_coef_relax, updated_basis_relax, s_list, method="reg_cv", init_grad=init_grad)
    relax_CI_DELTA  = compute_confidence_intervals(data, updated_coef_relax, updated_basis_relax, s_list, method="delta", eta_weak=1e-6)
    
    # Combine survival estimates into a DataFrame
    df_survival = pd.DataFrame({
        "time": df_surv_est_cv['t'],
        "CV_Selected": df_surv_est_cv['survival_curve'],
        "Targeted_Projection": df_surv_est_reg['survival_curve'],
        "Targeted_Delta": df_surv_est_delta['survival_curve'],
        "Targeted_Relaxed": df_surv_est_relax['survival_curve'],
        "Kaplan_Meier": km_df['KM']
    })
    
    # Return survival estimates and all confidence interval data frames:
    # Order: CV inference, then projection-based, delta-based, and relaxed-based.
    return (df_survival, 
            cv_CI_REG, cv_CI_REG_CV, cv_CI_DELTA,
            reg_CI_REG, reg_CI_REG_CV, reg_CI_DELTA, 
            delta_CI_REG, delta_CI_REG_CV, delta_CI_DELTA,
            relax_CI_REG, relax_CI_REG_CV, relax_CI_DELTA)

if __name__ == '__main__':
    import os
    import pickle
    from multiprocessing import Pool, cpu_count

    B = 40          # Number of simulations per batch/job
    n_samples_value = 2000
    num_jobs = 1#5      # Total number of jobs (each job runs B simulations)

    for job in range(num_jobs):
        job=job+4
        print(f"Starting job {job + 1}/{num_jobs}...")
        
        # Compute the starting index for the current job
        start_index = B * job
        args = [(sim + start_index, n_samples_value) for sim in range(B)]
        
        print(f"cpu count: {cpu_count()}")
        with Pool(processes=cpu_count()) as pool:
            results = pool.starmap(simulation_run, args)
        
        # Unpack the results from each simulation run
        survival_dfs         = [res[0] for res in results]
        cv_CI_REG_list       = [res[1] for res in results]
        cv_CI_REG_CV_list    = [res[2] for res in results]
        cv_CI_DELTA_list     = [res[3] for res in results]
        reg_CI_REG_list      = [res[4] for res in results]
        reg_CI_REG_CV_list   = [res[5] for res in results]
        reg_CI_DELTA_list    = [res[6] for res in results]
        delta_CI_REG_list    = [res[7] for res in results]
        delta_CI_REG_CV_list = [res[8] for res in results]
        delta_CI_DELTA_list  = [res[9] for res in results]
        relax_CI_REG_list    = [res[10] for res in results]
        relax_CI_REG_CV_list = [res[11] for res in results]
        relax_CI_DELTA_list  = [res[12] for res in results]
        
        # Save all results in a single dictionary
        results_dict = {
            "survival_dfs": survival_dfs,
            "cv_CI_REG": cv_CI_REG_list,
            "cv_CI_REG_CV": cv_CI_REG_CV_list,
            "cv_CI_DELTA": cv_CI_DELTA_list,
            "reg_CI_REG": reg_CI_REG_list,
            "reg_CI_REG_CV": reg_CI_REG_CV_list,
            "reg_CI_DELTA": reg_CI_DELTA_list,
            "delta_CI_REG": delta_CI_REG_list,
            "delta_CI_REG_CV": delta_CI_REG_CV_list,
            "delta_CI_DELTA": delta_CI_DELTA_list,
            "relax_CI_REG": relax_CI_REG_list,
            "relax_CI_REG_CV": relax_CI_REG_CV_list,
            "relax_CI_DELTA": relax_CI_DELTA_list
        }
        
        # Automatically create a filename that reflects the current batch (job)
        survival_filename = os.path.join(
            "out",
            f"part_{job + 1}_us_censored_1D_survival_results_TargetingGRID0.02_nSamples{n_samples_value}.pkl"
        )
        
        with open(survival_filename, 'wb') as f:
            pickle.dump(results_dict, f)
        
        print(f"Results saved as '{survival_filename}'.")
